{
  "analysis": "### Core Innovation\nThe creation of `OpenThoughts`, a series of high-quality, fully open-source datasets specifically designed for training advanced reasoning models, enabling transparent and reproducible research in the field.\n\n### Problem Addressed\nThe lack of progress and reproducibility in AI reasoning research due to the reliance of state-of-the-art models on proprietary, closed-source training data. This opacity hinders the community's ability to innovate.\n\n### Methodological Highlights\nA systematic, data-centric approach to building training recipes. The authors refined their data generation pipeline via 1,000+ controlled experiments and used a powerful teacher model (QwQ-32B) to generate a large-scale (1.2M examples) dataset.\n\n### Key Findings\n*   A model trained solely on their public `OpenThoughts2` data matched the performance of a strong model trained on proprietary data.\n*   \u26a1 The refined `OpenThoughts3` dataset enabled a smaller 7B model to achieve new state-of-the-art results on difficult reasoning benchmarks (AIME, LiveCodeBench, GPQA), significantly outperforming a comparable baseline.\n\n### Limitations & Open Questions\nThe methodology's reliance on a powerful \"teacher\" model raises questions about whether it primarily distills existing capabilities or can foster genuinely novel reasoning. The specific data generation components that proved most effective are not detailed.\n\n### Transferable Techniques\n*   **Systematic Data Pipeline Optimization**: Employ a large number of controlled experiments to rigorously test and improve each component of a data generation process.\n*   **High-Quality Knowledge Distillation**: Use a state-of-the-art \"teacher\" model to generate a scaled, high-quality dataset to train smaller, more efficient \"student\" models.\n*   **Iterative Dataset Engineering**: Treat the training dataset as a key artifact to be versioned and incrementally improved based on empirical model performance.",
  "arxiv": {
    "arxiv_id": "2506.04178v2",
    "title": "OpenThoughts: Data Recipes for Reasoning Models",
    "summary": "Reasoning models have made rapid progress on many benchmarks involving math,\ncode, and science. Yet, there are still many open questions about the best\ntraining recipes for reasoning since state-of-the-art models often rely on\nproprietary datasets with little to no public information available. To address\nthis, the goal of the OpenThoughts project is to create open-source datasets\nfor training reasoning models. After initial explorations, our OpenThoughts2-1M\ndataset led to OpenThinker2-32B, the first model trained on public reasoning\ndata to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as\nAIME and LiveCodeBench. We then improve our dataset further by systematically\ninvestigating each step of our data generation pipeline with 1,000+ controlled\nexperiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples\nand using QwQ-32B as teacher yields our OpenThoughts3-7B model, which achieves\nstate-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25,\nand 54% on GPQA Diamond - improvements of 15.3, 17.2, and 20.5 percentage\npoints compared to the DeepSeek-R1-Distill-Qwen-7B. All of our datasets and\nmodels are available on https://openthoughts.ai.",
    "authors": [
      "Etash Guha",
      "Ryan Marten",
      "Sedrick Keh",
      "Negin Raoof",
      "Georgios Smyrnis",
      "Hritik Bansal",
      "Marianna Nezhurina",
      "Jean Mercat",
      "Trung Vu",
      "Zayne Sprague",
      "Ashima Suvarna",
      "Benjamin Feuer",
      "Liangyu Chen",
      "Zaid Khan",
      "Eric Frankel",
      "Sachin Grover",
      "Caroline Choi",
      "Niklas Muennighoff",
      "Shiye Su",
      "Wanjia Zhao",
      "John Yang",
      "Shreyas Pimpalgaonkar",
      "Kartik Sharma",
      "Charlie Cheng-Jie Ji",
      "Yichuan Deng",
      "Sarah Pratt",
      "Vivek Ramanujan",
      "Jon Saad-Falcon",
      "Jeffrey Li",
      "Achal Dave",
      "Alon Albalak",
      "Kushal Arora",
      "Blake Wulfe",
      "Chinmay Hegde",
      "Greg Durrett",
      "Sewoong Oh",
      "Mohit Bansal",
      "Saadia Gabriel",
      "Aditya Grover",
      "Kai-Wei Chang",
      "Vaishaal Shankar",
      "Aaron Gokaslan",
      "Mike A. Merrill",
      "Tatsunori Hashimoto",
      "Yejin Choi",
      "Jenia Jitsev",
      "Reinhard Heckel",
      "Maheswaran Sathiamoorthy",
      "Alexandros G. Dimakis",
      "Ludwig Schmidt"
    ],
    "pdf_url": "http://arxiv.org/pdf/2506.04178v2",
    "html_url": "http://arxiv.org/abs/2506.04178v2",
    "published": "2025-06-04 17:25:39+00:00",
    "updated": "2025-06-05 02:21:52+00:00",
    "comment": "https://www.openthoughts.ai/blog/ot3. arXiv admin note: text overlap\n  with arXiv:2505.23754 by other authors",
    "primary_category": "cs.LG",
    "categories": [
      "cs.LG"
    ]
  }
}