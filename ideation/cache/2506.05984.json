{
  "analysis": "### Core Innovation\nUsing Audio-Aware Large Language Models (ALLMs) as automated, scalable judges to evaluate nuanced speaking styles (e.g., emotion, pace, pitch) in audio generated by Spoken Language Models (SLMs).\n\n### Problem Addressed\nThe high cost, subjectivity, and low scalability of human evaluation for assessing complex, non-textual qualities of AI-generated speech.\n\n### Methodological Highlights\nFour SLMs were tasked with voice style instruction-following and role-playing. The generated audio was evaluated by two ALLMs (GPT-4o-audio, Gemini-2.5-pro) and human judges, and the agreement rates were compared.\n\n### Key Findings\n*   \u26a1 The agreement between the Gemini-2.5-pro ALLM and human judges was comparable to the agreement between different human judges, validating its use as a reliable proxy.\n*   Even state-of-the-art SLMs like GPT-4o-audio have significant room for improvement in controlling speaking styles and generating natural dialogue.\n\n### Limitations & Open Questions\nThe study reveals performance gaps in current SLMs' generative capabilities more than limitations of the evaluation method. An open question is how to improve fine-grained stylistic control in speech synthesis models.\n\n### Transferable Techniques (\u22653 bullet points)\n*   Employing advanced AI models as reliable, automated judges for subjective, qualitative tasks to reduce reliance on human evaluators.\n*   Using \"instruction-following\" and \"role-playing\" as standardized tasks to benchmark the stylistic capabilities of generative models across different domains.\n*   Validating AI-based evaluation systems by measuring their agreement against inter-human agreement as a gold standard.",
  "arxiv": {
    "arxiv_id": "2506.05984v1",
    "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
    "summary": "Audio-aware large language models (ALLMs) can understand the textual and\nnon-textual information in the audio input. In this paper, we explore using\nALLMs as an automatic judge to assess the speaking styles of speeches. We use\nALLM judges to evaluate the speeches generated by SLMs on two tasks: voice\nstyle instruction following and role-playing. The speaking style we consider\nincludes emotion, volume, speaking pace, word emphasis, pitch control, and\nnon-verbal elements. We use four spoken language models (SLMs) to complete the\ntwo tasks and use humans and ALLMs to judge the SLMs' responses. We compare two\nALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and\nshow that the agreement between Gemini and human judges is comparable to the\nagreement between human evaluators. These promising results show that ALLMs can\nbe used as a judge to evaluate SLMs. Our results also reveal that current SLMs,\neven GPT-4o-audio, still have room for improvement in controlling the speaking\nstyle and generating natural dialogues.",
    "authors": [
      "Cheng-Han Chiang",
      "Xiaofei Wang",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Linjie Li",
      "Radu Kopetz",
      "Yao Qian",
      "Zhendong Wang",
      "Zhengyuan Yang",
      "Hung-yi Lee",
      "Lijuan Wang"
    ],
    "pdf_url": "http://arxiv.org/pdf/2506.05984v1",
    "html_url": "http://arxiv.org/abs/2506.05984v1",
    "published": "2025-06-06 11:05:48+00:00",
    "updated": "2025-06-06 11:05:48+00:00",
    "comment": null,
    "primary_category": "eess.AS",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL"
    ]
  }
}