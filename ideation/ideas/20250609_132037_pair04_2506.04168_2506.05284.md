# Mix-and-Match Ideation — 2506.04168 × 2506.05284

## Papers

### Horizon Reduction Makes RL Scalable

- arXiv ID: `2506.04168`
- URL: https://arxiv.org/abs/2506.04168

#### LLM Analysis
### Core Innovation
A minimal offline RL method, SHARSA (Scalable Horizon-Aware RL via Sarsa), that explicitly reduces the effective decision-making horizon to unlock scalability.

### Problem Addressed
Existing offline reinforcement learning algorithms exhibit poor scaling, failing to solve complex problems even when data, compute, and model capacity are massively increased. Their performance saturates well below optimal levels.

### Methodological Highlights
The study validates its hypothesis by testing algorithms on datasets up to 1000x larger than typical benchmarks. It uses controlled experiments to isolate the long horizon as the key barrier to scalability and compares SHARSA against other horizon reduction techniques.

### Key Findings
*   ⚡ Despite massive data scaling, many current offline RL algorithms fail to improve past a certain performance threshold.
*   Long planning horizons are a fundamental bottleneck preventing effective scaling in offline RL.
*   Techniques that explicitly reduce the effective horizon substantially enhance performance and scaling behavior.

### Limitations & Open Questions
The abstract does not specify the theoretical underpinnings for why long horizons are so detrimental. The generalizability of SHARSA and the optimal degree of horizon reduction for different tasks remain open questions.

### Transferable Techniques (≥3 bullet points)
*   Decompose long-horizon sequential decision problems by reducing the effective planning horizon to improve tractability.
*   When evaluating algorithm scalability, stress-test by scaling datasets far beyond standard benchmarks to reveal performance saturation points.
*   Instead of complex architectural changes, first investigate and address fundamental problem properties (like horizon length) as a path to scalability.

### Video World Models with Long-term Spatial Memory

- arXiv ID: `2506.05284`
- URL: https://arxiv.org/abs/2506.05284

#### LLM Analysis
### Core Innovation
A geometry-grounded long-term spatial memory framework for video world models, enabling them to recall and maintain consistency in previously generated environments.

### Problem Addressed
Standard autoregressive video models suffer from "forgetting" due to limited context windows. When an agent revisits a location, the model often fails to render it consistently, breaking the illusion of a persistent world.

### Methodological Highlights
The framework introduces explicit mechanisms to store generated scene information in a 3D spatial memory and retrieve it when a location is revisited. The authors also curated custom datasets designed to train and benchmark this long-term recall capability.

### Key Findings
Models equipped with the spatial memory demonstrate significant improvements in generation quality, scene consistency, and effective context length over baseline world models.

### Limitations & Open Questions
The abstract does not address the computational cost or scalability of the memory system. Its effectiveness in highly dynamic environments with many moving objects or changing geometry remains an open question.

### Transferable Techniques
*   Augmenting generative models with an explicit, external memory module to overcome limited context windows.
*   Grounding abstract model representations in a physical or geometric coordinate system (e.g., 3D space) to enforce real-world consistency.
*   Creating specialized, task-specific datasets to train and robustly evaluate niche model capabilities like long-term recall.

## Connections (Stage 2)

### 1. Conceptual Mapping
*   **Problem Equivalence:** The "long planning horizon" in RL (Paper A) is analogous to the "limited temporal context window" in generative models (Paper B); both represent a failure to handle long-term temporal dependencies.
*   **Solution Duality:** Paper A's "horizon reduction" simplifies the temporal problem, while Paper B's "long-term memory" extends the model's temporal reach.
*   **Underlying Failure:** Compounding errors in value estimation (A) maps to "forgetting" previously generated content (B).
*   **Methodological Approach:** Algorithmic simplification for scalability (A) corresponds to architectural augmentation for consistency (B).

### 2. Complementarity Matrix

| | **Strength: Long-Term Consistency (B)** | **Strength: Novel Architecture (B)** |
| :--- | :--- | :--- |
| **Strength: Scalability Focus (A)** | A's focus on scalability can ensure B's memory retrieval is efficient, preventing it from becoming a bottleneck in large worlds. | A's simple, scalable policy can be trained to explore the complex world generated by B's model more effectively. |
| **Strength: Fundamental Focus (A)** | A's principle of finding the core problem can refine B's memory, ensuring only essential information is stored for consistency. | B's memory architecture offers a direct solution to A's long-horizon problem by providing a "shortcut" to distant states. |

### 3. Synergy Hypotheses (≥3)
*   Combining a short-horizon policy (A) with a memory-augmented world model (B) can enable scalable agents to navigate persistent environments without complex credit assignment.
*   The spatial memory system (B) could serve as a terminal value function, providing stable targets for distant states to overcome the horizon limits identified in A.
*   An integrated agent could learn to dynamically switch between short-horizon reactive control (A) and long-horizon memory-guided planning (B) based on task demands.

### 4. Novelty & Feasibility Scores
```json
{
  "novelty": 9,
  "feasibility": 6
}
```

### 5. Risk Factors
*   **Integration Complexity:** Creating a stable and effective interface between the RL policy (A) and the generative memory-world model (B) is a significant engineering challenge.
*   **Objective Mismatch:** A short-horizon policy (A) may never learn to leverage the long-term memory (B) if local rewards don't incentivize it, rendering the memory module useless.
*   **Scalability Bottleneck:** The spatial memory system (B) could introduce its own computational costs that negate the scalability benefits gained from horizon reduction (A).

## Generated Ideas (Stage 3)

---
#### Idea 1: Terminal Value Networks: Using Spatial Memory for Long-Horizon Credit Assignment

**Research Abstract (≤ 60 words)**
We propose using a geometry-grounded spatial memory, not just for rendering, but as a non-parametric terminal value function for a short-horizon RL agent. This allows the agent to receive stable, long-term value estimates without complex temporal credit assignment, directly addressing the scalability bottleneck in long-horizon offline RL by providing a "shortcut" to distant state values.

**Date:** 2024-10-27
**Papers Inspiring This:** 2506.02134 & 2506.02891
**The Question**
Can a spatial memory system provide stable value estimates for distant states, allowing a short-horizon agent to solve long-horizon tasks efficiently?

**Why It's Interesting**
*   It elegantly solves the deep credit assignment problem by replacing error-prone value propagation with a memory lookup.
*   It creates a powerful new bridge between generative world models (perception/memory) and reinforcement learning (decision-making).
*   The approach could dramatically improve the sample efficiency and stability of RL in large, persistent worlds.

**Sketch of Approach**
*   Train a video world model with spatial memory (Paper B's method) on exploration data from a complex 3D environment.
*   Freeze the world model and use it as a simulator. Train a short-horizon agent (like SHARSA from Paper A).
*   When the agent's planning horizon is reached, query the spatial memory for the value of the terminal state and use this as a bootstrap target, instead of relying on the agent's own value function.

**Resources Needed**
*   Custom datasets for 3D navigation (like those from Paper B).
*   Significant GPU compute for world model and RL agent training.
*   Collaborators with expertise in both generative models and offline RL.

**Open Questions**
*   How should "value" be encoded and stored in the spatial memory?
*   Is the memory retrieval process fast enough to not become a new bottleneck during RL training?
*   Will this approach generalize to dynamic environments where the memory must be updated?

---
#### Idea 2: The Two-System Agent: Dynamically Switching Between Reactive Control and Memory-Guided Planning

**Research Abstract (≤ 60 words)**
We propose a dual-system agent that combines a fast, scalable, short-horizon policy for reactive control with a deliberative planner that uses a long-term spatial memory for navigation. A learned meta-controller arbitrates between these two systems, enabling the agent to be both highly efficient in simple situations and robustly consistent during complex, long-horizon tasks.

**Date:** 2024-10-27
**Papers Inspiring This:** 2506.02134 & 2506.02891
**The Question**
Can an agent learn to optimally switch between a computationally cheap reactive policy and a costly but more accurate memory-guided planner to maximize overall task performance?

**Why It's Interesting**
*   It mirrors cognitive science concepts of System 1 (fast, intuitive) and System 2 (slow, deliberate) thinking.
*   This architecture could lead to more flexible and computationally efficient agents that allocate resources intelligently.
*   It provides a clear path to integrating fundamentally different decision-making algorithms.

**Sketch of Approach**
*   Implement a short-horizon policy (Paper A) as the "reactive" system.
*   Implement a planner (e.g., A* search) that uses the spatial graph from the memory model (Paper B) as the "deliberative" system.
*   Train a high-level meta-controller using RL to decide which system to employ at each step, based on state features like model uncertainty or distance to goal.

**Resources Needed**
*   A simulated environment where this trade-off is meaningful (e.g., large maze with open areas).
*   Expertise in hierarchical reinforcement learning.
*   Compute for training three separate components (policy, planner, meta-controller).

**Open Questions**
*   What is the optimal state representation for the meta-controller to make its decision?
*   How do you manage the hand-off between the two control systems smoothly?
*   Is the overhead of the meta-controller decision process itself too costly?

---
#### Idea 3: Scalable Exploration for World Model Training via Horizon Reduction

**Research Abstract (≤ 60 words)**
Training consistent world models requires diverse, long-horizon data. We propose using a scalable, short-horizon exploration policy to efficiently generate this data. By rewarding local novelty, a simple agent can effectively cover large state spaces without complex planning, creating the rich datasets needed to train powerful world models with long-term spatial memory.

**Date:** 2024-10-27
**Papers Inspiring This:** 2506.02134 & 2506.02891
**The Question**
Can a simple, short-horizon policy serve as a more effective data collection engine for training complex generative world models than traditional exploration strategies?

**Why It's Interesting**
*   It addresses a key bottleneck in world model research: the collection of high-quality, diverse, and long-horizon training data.
*   It creates a virtuous cycle: a simple policy generates data for a better world model, which in turn can be used to train better policies.
*   This reframes the RL agent not as an end-user of a world model, but as a critical tool in its creation.

**Sketch of Approach**
*   Implement a short-horizon policy (Paper A) with an intrinsic motivation reward (e.g., state novelty or prediction error).
*   Deploy this agent in a complex, simulated environment (e.g., Habitat, ProcGen) to collect a large dataset of trajectories.
*   Train a memory-augmented world model (Paper B) on this dataset and compare its consistency and quality against a model trained on data from random exploration.

**Resources Needed**
*   Rich simulation environments.
*   Large-scale compute for parallelized data collection and world model training.
*   Offline datasets for comparison.

**Open Questions**
*   Can a short-horizon policy avoid getting stuck in "novelty traps," failing to achieve global exploration?
*   Does the data generated by the policy have biases that negatively impact the final world model?
*   How does this compare to more complex, state-of-the-art exploration methods?

---
#### Idea 4: Memory-Grounded Curricula for Reinforcement Learning

**Research Abstract (≤ 60 words)**
We propose using a pre-populated spatial memory to generate an automatic curriculum for an RL agent. By framing the problem as a series of short-horizon sub-tasks—navigating between known locations in memory—we can decompose a single intractable long-horizon task into many solvable ones, dramatically simplifying credit assignment and accelerating learning for a scalable agent.

**Date:** 2024-10-27
**Papers Inspiring This:** 2506.02134 & 2506.02891
**The Question**
Can we leverage a world model's spatial memory to automatically generate a curriculum of short-horizon sub-tasks that enables a simple agent to solve complex, long-horizon problems?

**Why It's Interesting**
*   It automates the difficult process of curriculum design and sub-goal setting.
*   It provides a principled way to combine the strengths of world modeling (knowledge representation) and RL (behavior optimization).
*   This could enable agents to solve tasks that are currently impossible with end-to-end training.

**Sketch of Approach**
*   First, allow an agent to explore an environment to populate a spatial memory map (using Paper B's method).
*   Define a curriculum of sub-tasks where the start and end points are nodes in the memory graph.
*   Train a short-horizon agent (Paper A) on this curriculum of "memory-to-memory" traversals.
*   Evaluate the agent's zero-shot performance on the full, long-horizon task against a baseline agent trained without the curriculum.

**Resources Needed**
*   A 3D navigation environment.
*   A pre-trained or trainable memory-augmented world model.
*   Framework for managing curriculum-based RL training.

**Open Questions**
*   Will the agent overfit to the curriculum and fail to generalize to paths not defined by memory nodes?
*   How do you select the optimal sequence of sub-tasks from the memory graph?
*   Is this approach robust to inaccuracies or sparsity in the initial spatial memory?

---
#### Idea 5: Using Scalable Agents to Quantify the Economic Cost of Inconsistency in Generative Models

**Research Abstract (≤ 60 words)**
How much does a world model's inconsistency matter for downstream tasks? We propose using a scalable offline RL agent as a standardized "probe" to quantify this. By measuring the performance gap of an agent trained on data from consistent versus inconsistent world models, we can establish a task-based, economic metric for the value of long-term memory.

**Date:** 2024-10-27
**Papers Inspiring This:** 2506.02134 & 2506.02891
**The Question**
Can we develop a principled, task-based metric to quantify the performance degradation an RL agent suffers due to a world model's long-term inconsistency?

**Why It's Interesting**
*   It moves beyond pixel-based metrics (like PSNR/SSIM) to evaluate generative models based on their utility for a downstream agent.
*   It provides a concrete method to justify the computational expense of complex architectures like spatial memory.
*   This could help guide research by identifying which types of model failures are most detrimental to decision-making.

**Sketch of Approach**
*   Generate two large, offline datasets from a controlled environment. Dataset 1 uses a baseline world model that "forgets." Dataset 2 uses a memory-augmented world model (Paper B) that is consistent.
*   Train identical, highly scalable offline RL agents (like SHARSA from Paper A) on both datasets.
*   The difference in the asymptotic performance of the two agents represents the "economic cost of forgetting."

**Resources Needed**
*   Access to both baseline and memory-augmented generative world models.
*   Large-scale offline RL datasets.
*   Significant compute for training multiple RL agents to convergence.

**Open Questions**
*   Is the performance gap highly sensitive to the choice of the probe RL agent?
*   How does the "cost of inconsistency" vary across different tasks (e.g., navigation vs. manipulation)?
*   Could this metric be fooled if the agent learns to exploit specific, predictable model flaws?
