title,abstract,central_question,approach_summary,resources_needed,paper_a_id,paper_b_id,date_generated
The Self-Architecting Agent: Metacognitive Pruning of Neural Modules,"We explore agents that autonomously reshape their own neural architecture over their lifetime. Using a metacognitive framework, the agent identifies and proposes to prune under-utilized components—beyond just attention heads—based on self-generated relevance scores. This enables continuous, resource-aware self-improvement and adaptation, pushing AI beyond simple weight updates.",Can an agent use metacognitive self-assessment to safely and effectively prune its own architectural components to adapt to new domains or resource constraints?,"*   Generalize SparseMM's relevance scoring to other module types (e.g., FFN layers, vision encoder blocks).
*   Implement a metacognitive loop that proposes a ""pruning plan"" for a set of tasks.
*   Evaluate the plan in a sandboxed environment, measuring performance impact before committing the architectural change.","Pre-trained MLLMs, diverse multimodal datasets, significant compute for re-evaluation cycles.",2506.05344,2506.05109,2025-06-09
The Metacognitive Budget: Prioritizing Learning via Self-Assessment,"We propose a learning paradigm where an agent allocates its own ""learning budget"" (e.g., gradient updates) to its internal components. Using metacognitive self-assessment inspired by relevance scoring, the agent identifies which parts of itself need the most improvement for a given task, enabling more efficient and targeted continual learning.",Can an agent accelerate its own learning and reduce forgetting by intelligently allocating training resources to different parts of its neural network?,"*   During a forward pass, use a relevance scoring method to identify modules critical to a task but producing high error.
*   In the backward pass, asymmetrically scale learning rates or selectively apply gradients based on these scores.
*   Evaluate on continual learning benchmarks to measure knowledge acquisition speed and forgetting rates.","Continual learning benchmarks (e.g., C-Imagenet), standard deep learning frameworks, MLLMs.",2506.05344,2506.05109,2025-06-09
Sparsity as a Mirror: A Quantitative Metric for Agent Metacognition,"We propose a novel method to evaluate an agent's intrinsic metacognition. We hypothesize that a well-developed metacognitive agent will exhibit highly stable and task-specific computational sparsity patterns. By analyzing the dynamics of these patterns with relevance scoring techniques, we can create a quantitative, behavioral metric for an agent's self-awareness.","Can the stability and specificity of emergent computational sparsity serve as a reliable, quantitative proxy for an agent's underlying metacognitive ability?","*   Develop several agents with varying levels of metacognitive ability (from none to a full implementation).
*   Subject them to a diverse set of tasks and track their internal relevance scores over time.
*   Measure the consistency and task-appropriateness of the resulting sparsity patterns as the core metric.","Multiple MLLM models, compute for extensive testing, expertise in cognitive science for validation.",2506.05344,2506.05109,2025-06-09
The Metacognitive Co-Pilot: Human-AI Collaborative Self-Optimization,"To mitigate the risks of fully autonomous self-modification, we propose a human-in-the-loop metacognitive system. The agent uses relevance analysis to identify potential efficiency optimizations (e.g., head pruning) and presents them, along with a generated ""impact report,"" to a human expert for approval. This balances autonomous improvement with human oversight for alignment and safety.","Can a human-in-the-loop system, where an agent proposes architectural changes and a human approves them, achieve safe and aligned self-optimization?","*   Build a UI where an agent can propose optimizations derived from SparseMM-style analysis.
*   The agent must also generate a natural language ""impact report"" (e.g., ""Pruning these heads may slightly reduce performance on abstract reasoning tasks"").
*   A human expert reviews the proposal and its justification, then approves or denies the change.","MLLM, UI/UX design expertise, human experts for the loop, interpretability tools.",2506.05344,2506.05109,2025-06-09
The Sparsity Oracle: A Metacognitive Module for Self-Analysis,"We will build a lightweight ""metacognitive module"" for LLMs. This module will leverage the training-free analysis from SparseMM to continuously generate an explicit ""map"" of its own internal specializations (e.g., which heads handle syntax, semantics, or vision). This map provides interpretable self-knowledge for downstream tasks like debugging, routing, and model merging.","Can a cheap, training-free analysis method serve as a continuous source of ""metacognitive knowledge"" for an AI agent, enabling it to understand its own internal structure?","*   Generalize SparseMM's response analysis to probe for various concepts beyond just ""visual.""
*   Package this analysis tool as a plug-and-play module for standard LLMs.
*   Demonstrate utility by showing the generated map can improve a downstream task, like routing queries to specialized sub-networks.","Pre-trained LLMs, curated probe datasets for various skills, compute for extensive analysis runs.",2506.05344,2506.05109,2025-06-09
Learning to Prune: Adaptive Sparsity Thresholding via Meta-Learning,"SparseMM uses a fixed, empirical threshold for identifying visual heads. We propose a meta-learning approach where an agent learns an optimal, context-dependent sparsity threshold. The agent will use metacognitive evaluation (reflecting on task outcomes) to adjust its pruning aggressiveness, maximizing the accuracy-efficiency trade-off across diverse multimodal tasks and solving a key limitation of the original work.","Can a metacognitive evaluation loop learn the optimal sparsity threshold for a technique like SparseMM, adapting it to different tasks and data distributions?","*   Frame the threshold selection as a simple reinforcement learning or multi-armed bandit problem.
*   The agent's action is to select a sparsity threshold (e.g., 5%, 10%) for the next batch of tasks.
*   The reward is the task accuracy minus a penalty for computational cost.","SparseMM implementation, multimodal benchmarks (e.g., MMBench), moderate GPU compute.",2506.05344,2506.05109,2025-06-09
The Frugal Agent: Metacognitive Regulation of Computational Resources,"We aim to build an agent that actively manages its own computational budget. Using metacognitive planning, the agent will first assess task difficulty and then decide how much of its ""brain"" (e.g., number of active attention heads) to allocate. This integrates SparseMM’s resource allocation mechanism into a metacognitive control loop, enabling ""Green AI"" by default.",Can an agent learn to allocate its own computational resources based on a metacognitive pre-assessment of task difficulty?,"*   Create a lightweight ""difficulty assessment"" module that scores an input prompt.
*   Use this score to inform a planner that selects a compute profile (defined by head sparsity).
*   Evaluate the agent's ability to use minimal resources on easy tasks while scaling up for hard ones.","Pre-trained MLLM, datasets with varying difficulty (e.g., simple vs. complex VQA), inference servers for profiling.",2506.05344,2506.05109,2025-06-09
"A Universal ""Concept Sparsity"" Hypothesis for LLMs","SparseMM found that ~5% of heads are ""visual."" We hypothesize this is a specific instance of a general ""concept sparsity"" principle: for any concept (math, coding, poetry), a small, identifiable subset of heads is primarily responsible. We will test this by extending SparseMM's analysis to non-visual domains to map functional specialization in LLMs.","Does the ""visual head"" sparsity phenomenon generalize to other cognitive domains like logical reasoning, programming, or creative writing?","*   Create targeted probe datasets for different domains (code, math, etc.).
*   Apply SparseMM's training-free response analysis to identify ""coding heads,"" ""math heads,"" etc., in a base LLM.
*   Validate by showing a performance drop when pruning these heads on in-domain vs. out-of-domain tasks.","Strong base LLMs (Llama 3, GPT-4), domain-specific benchmarks (HumanEval, GSM8K), analysis compute.",2506.05344,2506.05109,2025-06-09
The Decomposition Benchmark Generator,"We propose creating a new benchmark, ""DecompBench,"" using fictional data generation to systematically test the limits of automated task decomposition. By procedurally generating problems with controlled levels of sub-task inter-dependency and complexity, we can rigorously evaluate and help improve reasoning frameworks like R2-Reasoner, specifically addressing their performance on tasks that are inherently difficult to decompose.",Can we programmatically generate complex reasoning tasks with controlled decomposability to benchmark and identify the failure modes of hierarchical reasoning systems?,"*   Extend the fictional data generator with parameters controlling task decomposability (e.g., number of dependent sub-questions, required reasoning hops).
*   Generate a suite of problems ranging from easily separable to highly entangled.
*   Benchmark R2-Reasoner and other decomposition-based methods on this suite to create a public leaderboard.","*   Advanced synthetic data generator.
*   Compute for large-scale benchmark generation.
*   Collaboration with developers of other reasoning frameworks.",2506.05901,2506.05639,2025-06-09
The Recursive Narrative Generator,"To overcome the bottleneck of creating high-quality synthetic data, we propose inverting the synergy: using an R2-Reasoner-like framework to *generate* the fictional narratives. Simple tasks like describing a room will be routed to cheap models, while complex tasks like developing a plot twist will be routed to powerful models, enabling scalable, cost-effective, and coherent story generation.","Can a heterogeneous model framework be used to generate complex, coherent fictional narratives more cost-effectively than a single monolithic LLM?","*   Frame narrative generation as a decomposable task (e.g., outline, character bios, scene-by-scene generation).
*   Train or configure a router to allocate these sub-tasks to models of appropriate capability.
*   Evaluate the generated narratives on coherence, quality, and cost compared to a single-LLM baseline.","*   A pre-trained model router.
*   Compute for generation and fine-tuning.
*   Human evaluators for assessing narrative quality.",2506.05901,2506.05639,2025-06-09
Reinforcement Learning for Factual Grounding,"We propose refining the R2-Reasoner's training by using a reward signal that explicitly optimizes for factual correctness within a controlled environment. By leveraging a fictional dataset where ground truth is absolute, we can train the model router via reinforcement learning to prioritize accuracy, potentially learning to allocate more powerful models to fact-critical sub-tasks and mitigate hallucination.","Can a reinforcement learning signal derived from a closed-world, fictional knowledge base train a model router to explicitly minimize factual errors in its final output?","*   Use the fictional Q&A dataset as the training environment.
*   Modify the RL reward function in the R2-Reasoner pipeline to heavily penalize factual inaccuracies in answers.
*   Measure the rate of factual errors and compare it to the baseline model trained only for cost/accuracy.","*   R2-Reasoner's RL training pipeline.
*   A large-scale fictional Q&A dataset.
*   Significant compute for RL experiments.",2506.05901,2506.05639,2025-06-09
A Router for Originality,"We propose a system that routes information requests based on whether they require verbatim recall or novel synthesis. Using an augmented fictional dataset, we will train a router to distinguish these two processes, sending verbatim queries to a simple lookup function and synthesis queries to a generative LLM. This provides a mechanism to control for originality and prevent plagiarism.","Can a model router be trained to identify and separate sub-tasks solvable by verbatim recall from those requiring novel synthesis, routing them to different processors?","*   Augment the fictional dataset with pairs that explicitly test verbatim recall vs. synthesis.
*   Train the R2-Reasoner with a policy that rewards routing verbatim tasks to a cheap model/database and synthesis tasks to a powerful LLM.
*   Evaluate the system's ability to generate novel text while correctly retrieving exact information when asked.","*   An augmented synthetic dataset.
*   The R2-Reasoner framework.
*   Metrics for textual novelty (e.g., n-gram overlap with training data).",2506.05901,2506.05639,2025-06-09
The Adaptive Citer: Principle-Driven Sourcing for Conversational Search,"Current search LLMs use static sourcing strategies. We propose an agent that dynamically adapts its information sources (e.g., encyclopedias vs. forums) based on conversational intent, guided by a principle-following reward model. Trained on Search Arena, it will switch principles on-the-fly (e.g., ""prioritize academic sources"" vs. ""prefer community platforms"") to improve response relevance.",Can a reward model learn to dynamically switch between information sourcing principles based on conversational context to improve user satisfaction?,"*   Classify conversations in Search Arena by user intent (e.g., factual lookup, opinion seeking).
*   Formulate sourcing principles for each intent (e.g., ""For troubleshooting, prefer forum discussions"").
*   Train a RewardAnything model to apply the appropriate principle based on the classified intent.","*   Search Arena dataset
*   A query intent classifier
*   RewardAnything model architecture",2506.03637,2506.05334,2025-06-09
The Preference-to-Principle Pipeline: A Semi-Automated Workflow for Rapid LLM Re-alignment,"We propose a semi-automated pipeline to accelerate LLM alignment. The system would first analyze a preference dataset like Search Arena to identify user heuristics. It would then assist a human operator in formulating a natural language principle to counteract this bias, which is then fed directly into a dynamic reward model for immediate re-alignment without retraining.","Can we create a human-in-the-loop workflow that rapidly translates insights from preference data into actionable principles for dynamic RMs, closing the loop from analysis to alignment?","*   Build a tool to surface statistical biases in the Search Arena dataset.
*   Create a UI for a human expert to write a corrective principle based on the tool's output.
*   Integrate this principle into a RewardAnything model and demonstrate the alignment shift.","*   Search Arena dataset
*   UI/software development resources
*   RewardAnything model architecture",2506.03637,2506.05334,2025-06-09
Principled Exploration: Using Dynamic RMs to Guide Diverse Preference Data Collection,"Collecting diverse preference data is a bottleneck. We propose using a principle-driven RM to guide response generation during data collection. By systematically varying principles (e.g., ""be creative,"" ""be cautious,"" ""be concise""), we can elicit a wider, more controlled range of LLM behaviors, creating a richer and more balanced dataset than passive collection allows.","Can we use a dynamically-principled reward model to actively guide an LLM to explore diverse behavioral modes, thereby creating more efficient and comprehensive preference datasets?","*   Define a set of orthogonal behavioral principles (e.g., conciseness, creativity, formality).
*   Use a RewardAnything-style model to guide a generator LLM to produce paired responses, where each response optimizes for a different principle.
*   Collect human preferences on these actively-generated, diverse pairs.","*   Generator LLM
*   RewardAnything architecture
*   Crowd-sourcing platform/budget",2506.03637,2506.05334,2025-06-09
Preference Vector Models: Deconstructing and Recombining Principles for Complex Alignment,"User preference is a composite of factors like accuracy, tone, and conciseness. We propose training a reward model to respond to a *vector* of principles simultaneously. Using Search Arena, we will deconstruct aggregate user preferences into a basis set of core principles, allowing for fine-grained, compositional control over LLM alignment by adjusting the weights of this vector.","Can a complex, implicit user preference function be decomposed into a weighted combination of simpler, explicit natural language principles for more nuanced model control?","*   Define a basis set of principles (e.g., P_accuracy, P_conciseness, P_formality).
*   Train a multi-principle RM on Search Arena, learning weights for each principle that best predict aggregate human preference.
*   Test if recombining these principles with new weights can generate novel, desirable behaviors.","*   Search Arena dataset
*   Significant compute for a more complex, multi-input RM architecture
*   Expertise in multi-objective optimization",2506.03637,2506.05334,2025-06-09
Modular Speech Synthesis via Style Routing,"We propose a ""speech router"" that decomposes a high-level stylistic prompt (e.g., ""speak like a tired wizard"") into atomic sub-tasks like pitch, pace, and prosody. Each sub-task is routed to a specialized, efficient model. The outputs are then synthesized into a single, coherent audio stream, enabling fine-grained and cost-effective control over generated speech.",Can we achieve more controllable and efficient expressive speech synthesis by decomposing stylistic requests and routing them to a heterogeneous set of specialized generative models?,"*   Train a ""style decomposer"" LLM to break down text prompts into a structured set of parameters (e.g., JSON with pitch_avg, pace_wpm, emotion_label).
*   Train or fine-tune lightweight models, each an expert in one parameter (e.g., a pace-control model).
*   Develop a synthesizer module to combine the outputs from the expert models into a final waveform.","*   Speech datasets with rich stylistic labels, compute for training multiple small models, expertise in signal processing and speech synthesis.",2506.05901,2506.05984,2025-06-09
The AI Sieve: Cascaded Evaluation for Generative Media,"We introduce a cascaded evaluation framework that routes generated media through a series of increasingly powerful and expensive AI judges. Simple models first filter out obviously flawed outputs, reserving sophisticated ALLMs for nuanced cases. This ""AI Sieve"" dramatically reduces evaluation costs and latency while maintaining high standards for quality assurance in large-scale generative pipelines.","Can a multi-stage, routed evaluation pipeline significantly reduce the cost and latency of quality assurance for generative AI without sacrificing accuracy?","*   Define a hierarchy of evaluation tasks (e.g., L1: Is it intelligible? L2: Does it match the emotion?).
*   Train or select a cheap model for each lower-level task.
*   Build a router that passes an audio sample down the cascade, flagging it if it fails a stage, or passing it to the final ALLM judge if it passes all prior checks.","*   A benchmark dataset of generated audio with known flaws, access to various speech models, a dataset for training the router policy.",2506.05901,2506.05984,2025-06-09
Self-Correcting AI Judges via Human Consensus Alignment,"We propose a method to improve AI judges by treating their agreement with human consensus as a reward signal. An AI judge's evaluation model is fine-tuned using reinforcement learning to maximize its alignment with a ""ground truth"" derived from inter-human agreement scores. This creates a self-correcting system for more reliable, human-aligned, and trustworthy automated evaluation.","Can we use inter-human agreement as a reward signal to automatically fine-tune an AI judge, making it more aligned with the nuances of human perception over time?","*   Collect a dataset of generated audio evaluated by multiple human raters.
*   Calculate inter-human agreement (e.g., Krippendorff's alpha) to establish a ""consensus quality"" score.
*   Frame this as an RL problem where the AI judge is the agent and the reward is a function of its agreement with the human consensus, then fine-tune it.","*   Datasets with multi-rater human evaluations, a parameter-efficient fine-tunable AI judge, compute for the RL tuning loop.",2506.05901,2506.05984,2025-06-09
Adversarial Co-evolution of Speech Generators and Judges,"We propose an adversarial framework where a generative Spoken Language Model (SLM) and an Audio-Aware Large Language Model (ALLM) judge are trained simultaneously. The SLM is trained to produce speech that fools the judge, while the judge is concurrently trained to better distinguish the SLM's output from real human speech, driving mutual, open-ended improvement in both models.",Can a high-level adversarial training loop between a generator (SLM) and a discriminator (ALLM Judge) lead to a new state-of-the-art in both expressive speech synthesis and its evaluation?,"*   Initialize an SLM generator and an ALLM judge.
*   In an alternating loop: (a) train the SLM to maximize the judge's ""human-likeness"" score; (b) train the judge to better differentiate between the SLM's output and real human speech.
*   Use parameter-efficient fine-tuning methods to make the training tractable.","*   Massive compute, expertise in stabilizing adversarial training, a large corpus of human speech, state-of-the-art PEFT methods.",2506.05901,2506.05984,2025-06-09
The Two-System Agent: Dynamically Switching Between Reactive Control and Memory-Guided Planning,"We propose a dual-system agent that combines a fast, scalable, short-horizon policy for reactive control with a deliberative planner that uses a long-term spatial memory for navigation. A learned meta-controller arbitrates between these two systems, enabling the agent to be both highly efficient in simple situations and robustly consistent during complex, long-horizon tasks.",Can an agent learn to optimally switch between a computationally cheap reactive policy and a costly but more accurate memory-guided planner to maximize overall task performance?,"*   Implement a short-horizon policy (Paper A) as the ""reactive"" system.
*   Implement a planner (e.g., A* search) that uses the spatial graph from the memory model (Paper B) as the ""deliberative"" system.
*   Train a high-level meta-controller using RL to decide which system to employ at each step, based on state features like model uncertainty or distance to goal.","*   A simulated environment where this trade-off is meaningful (e.g., large maze with open areas).
*   Expertise in hierarchical reinforcement learning.
*   Compute for training three separate components (policy, planner, meta-controller).",2506.04168,2506.05284,2025-06-09
Scalable Exploration for World Model Training via Horizon Reduction,"Training consistent world models requires diverse, long-horizon data. We propose using a scalable, short-horizon exploration policy to efficiently generate this data. By rewarding local novelty, a simple agent can effectively cover large state spaces without complex planning, creating the rich datasets needed to train powerful world models with long-term spatial memory.","Can a simple, short-horizon policy serve as a more effective data collection engine for training complex generative world models than traditional exploration strategies?","*   Implement a short-horizon policy (Paper A) with an intrinsic motivation reward (e.g., state novelty or prediction error).
*   Deploy this agent in a complex, simulated environment (e.g., Habitat, ProcGen) to collect a large dataset of trajectories.
*   Train a memory-augmented world model (Paper B) on this dataset and compare its consistency and quality against a model trained on data from random exploration.","*   Rich simulation environments.
*   Large-scale compute for parallelized data collection and world model training.
*   Offline datasets for comparison.",2506.04168,2506.05284,2025-06-09
Memory-Grounded Curricula for Reinforcement Learning,"We propose using a pre-populated spatial memory to generate an automatic curriculum for an RL agent. By framing the problem as a series of short-horizon sub-tasks—navigating between known locations in memory—we can decompose a single intractable long-horizon task into many solvable ones, dramatically simplifying credit assignment and accelerating learning for a scalable agent.","Can we leverage a world model's spatial memory to automatically generate a curriculum of short-horizon sub-tasks that enables a simple agent to solve complex, long-horizon problems?","*   First, allow an agent to explore an environment to populate a spatial memory map (using Paper B's method).
*   Define a curriculum of sub-tasks where the start and end points are nodes in the memory graph.
*   Train a short-horizon agent (Paper A) on this curriculum of ""memory-to-memory"" traversals.
*   Evaluate the agent's zero-shot performance on the full, long-horizon task against a baseline agent trained without the curriculum.","*   A 3D navigation environment.
*   A pre-trained or trainable memory-augmented world model.
*   Framework for managing curriculum-based RL training.",2506.04168,2506.05284,2025-06-09
Using Scalable Agents to Quantify the Economic Cost of Inconsistency in Generative Models,"How much does a world model's inconsistency matter for downstream tasks? We propose using a scalable offline RL agent as a standardized ""probe"" to quantify this. By measuring the performance gap of an agent trained on data from consistent versus inconsistent world models, we can establish a task-based, economic metric for the value of long-term memory.","Can we develop a principled, task-based metric to quantify the performance degradation an RL agent suffers due to a world model's long-term inconsistency?","*   Generate two large, offline datasets from a controlled environment. Dataset 1 uses a baseline world model that ""forgets."" Dataset 2 uses a memory-augmented world model (Paper B) that is consistent.
*   Train identical, highly scalable offline RL agents (like SHARSA from Paper A) on both datasets.
*   The difference in the asymptotic performance of the two agents represents the ""economic cost of forgetting.""","*   Access to both baseline and memory-augmented generative world models.
*   Large-scale offline RL datasets.
*   Significant compute for training multiple RL agents to convergence.",2506.04168,2506.05284,2025-06-09
Compositional Flow,"This research uses a language model to decompose complex text prompts into a sequence of simpler sub-prompts. Each sub-prompt guides a STARFlow model to generate a specific element or region of an image. This ""divide and conquer"" approach aims to improve compositional accuracy and object relationships in complex scenes, a known weakness of generative models.",Can we improve the compositional correctness of generated images by first decomposing the prompt into a semantic scene graph and generating components sequentially?,"*   Use an LLM as a task decomposer to turn a prompt into a scene layout or sequence of generation steps.
*   Adapt the STARFlow model to accept sequential or masked guidance inputs for each step.
*   Evaluate on a compositional benchmark like T2I-CompBench to measure gains in object relations.","Powerful LLM for decomposition, pre-trained STARFlow model, compositional evaluation benchmarks, compute for fine-tuning.",2506.05901,2506.06276,2025-06-09
Fractal Reasoners,"Inspired by STARFlow's efficient architecture, we propose restructuring the model pool in routing frameworks like R2-Reasoner. Instead of a few discrete models (one SLM, one LLM), we will create a ""fractal"" hierarchy of deep-shallow language models. A router can then select from this finer-grained continuum, enabling more precise matching of model capacity to sub-task difficulty.","Does a more granular, architecturally-varied pool of language models improve the efficiency and accuracy of a dynamic reasoning router?","*   Create variants of a base LLM (e.g., Llama) using the deep-shallow principle.
*   Integrate this new, granular model pool into the R2-Reasoner framework.
*   Benchmark on reasoning tasks (e.g., GSM8K) to measure cost/accuracy improvements over a simple two-model baseline.","Base open-source LLM, compute for architectural variants training/fine-tuning, reasoning benchmarks.",2506.05901,2506.06276,2025-06-09
Hybrid-Medium Synthesis,"We propose a ""meta-generator"" that decomposes an image prompt by modality and routes components to specialized generative models. For instance, a STARFlow model could generate the overall structure, while a diffusion model handles photorealistic textures. This leverages the unique strengths of different architectures to create a composite image superior to what any single model could achieve.","Can we generate higher-fidelity images by routing different semantic components of a prompt to different, specialized generative architectures within a single workflow?","*   Use an LLM decomposer to tag parts of a prompt (e.g., ""structure,"" ""texture,"" ""face"").
*   Train a router to dispatch these tagged sub-tasks to a pool of models (STARFlow, Stable Diffusion, StyleGAN).
*   Develop a final ""stitching"" model to combine the outputs into a coherent image.","Pool of diverse, pre-trained generative models; powerful LLM for routing; compute for training the stitching model.",2506.05901,2506.06276,2025-06-09
Latent Space Orchestrator,"Instead of routing prompts to different models, we propose an internal router that directs semantic information *within* a single, large STARFlow model. A controller would parse the prompt and dynamically allocate concepts (e.g., ""cat,"" ""background"") to different layers of the model's transformer blocks, enabling more specialized and disentangled internal processing of the latent space.",Can we improve generation quality by dynamically routing semantic concepts to specialized computational pathways within a single monolithic generative model?,"*   Augment the STARFlow architecture with a small, lightweight routing module (e.g., a cross-attention mechanism).
*   Train the router to map parsed prompt entities to specific Transformer block ranges.
*   Fine-tune the entire model end-to-end to leverage this specialized internal processing.","Access to STARFlow codebase and weights, compute for architectural modification and fine-tuning, datasets with rich semantic annotations.",2506.05901,2506.06276,2025-06-09
The Multimodal Search Arena: Benchmarking MLLMs in Image-and-Text Grounded Dialogue,"We propose creating the ""Multimodal Search Arena,"" extending the original dataset to include image search results. This benchmark will be used to evaluate different MLLM fusion architectures, as categorized in Paper B, on their ability to synthesize and ground conversational responses in both textual and visual information retrieved from the web, providing a much-needed real-world testbed.","How do different multimodal fusion architectures (e.g., early vs. late fusion) perform in a realistic, conversational, multimodal search-and-retrieval task?","*   Develop a data collection pipeline where users interact with an MLLM that can retrieve and display both text and images.
*   Collect paired comparisons and full system traces, analogous to the original Search Arena methodology.
*   Benchmark a set of MLLMs representing different fusion strategies from Paper B's taxonomy.","Significant data collection infrastructure, crowd-sourcing budget, access to multiple MLLM APIs/models.",2506.05334,2506.04788,2025-06-09
Preference-Driven Fusion: Optimizing MLLM Architectures Directly on Human Feedback,"This project will use the paired-comparison methodology from Search Arena as a direct reward signal to optimize MLLM fusion strategies. We will investigate how different architectures from Paper B's taxonomy learn when trained via preference-based reinforcement learning (e.g., DPO), aiming to discover architectures that are not just technically sound but are also demonstrably preferred by humans in complex tasks.",Can we leverage direct human preference data from a live environment to automatically discover and refine more effective multimodal fusion architectures?,"*   Implement two or three distinct fusion architectures (from Paper B's taxonomy) in a Search Arena-style environment.
*   Collect user preference data (i.e., ""Model A's response was better than Model B's"").
*   Use the collected preference pairs to fine-tune the models using an algorithm like DPO.","Search Arena dataset, significant compute for RL/DPO training, live model serving infrastructure for data collection.",2506.05334,2506.04788,2025-06-09
Deconstructing Trust: Identifying User Heuristics in Multimodal AI Systems,"Paper A found citation count is a powerful but misleading heuristic for user trust. We will investigate the equivalent heuristics for other modalities. This study will use controlled experiments to determine what features of image, audio, or video integration (e.g., image resolution, source attribution, audio clarity) most influence a user's perceived credibility in MLLM outputs.","What are the superficial, non-semantic features that act as proxies for user trust when an LLM integrates non-textual modalities like images or audio?","*   Design a series of controlled A/B tests presenting users with MLLM outputs grounded in images or audio.
*   Systematically vary non-semantic features like image source (e.g., stock photo vs. user-generated), resolution, or placement within the text.
*   Measure user preference and stated trustworthiness to isolate the causal impact of each feature.","HCI/UX research expertise, platform for controlled experiments, crowd-sourcing budget.",2506.05334,2506.04788,2025-06-09
The Modality Brittleness Test: Quantifying MLLM Robustness to Missing Inputs,"Inspired by the ""cross-arena"" analysis in Paper A, we propose a systematic study of modality brittleness in MLLMs. We will evaluate state-of-the-art MLLMs on text-only tasks, and vice-versa, to quantify performance degradation when an expected input modality is absent. This will reveal how tightly coupled, and thus brittle, current fusion architectures are.",How robust are current Multimodal LLM fusion architectures when one or more of their expected input modalities are missing or irrelevant to the task?,"*   Select several MLLMs from Paper B's taxonomy representing different fusion strategies (e.g., early vs. late).
*   Evaluate their baseline performance on standard multimodal benchmarks (e.g., VQA).
*   Re-evaluate the same models on text-only benchmarks (e.g., MMLU), providing null/empty image tokens, and measure the performance drop.","Access to various MLLM APIs or open-source checkpoints, compute for running evaluations on a suite of benchmarks.",2506.05334,2506.04788,2025-06-09
